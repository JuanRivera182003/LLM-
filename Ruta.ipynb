{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8UGGooUS-rq"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27329,
     "status": "ok",
     "timestamp": 1751519893775,
     "user": {
      "displayName": "juan andres rivera gutierrez",
      "userId": "00082920026588442285"
     },
     "user_tz": 240
    },
    "id": "amzQ5qtJu0Zc",
    "outputId": "2c10972c-1f4c-4604-90b3-52391348d328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "✅ Guardado: /content/drive/My Drive/LLM/dataset_with_slots.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def extract_slots(tokens, labels):\n",
    "    slots = []\n",
    "    current = {}\n",
    "    current_key = \"\"\n",
    "    buffer = []\n",
    "\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab.startswith(\"B-\"):\n",
    "            # Si venía acumulando algo, guardar\n",
    "            if current_key and buffer:\n",
    "                current[current_key] = \" \".join(buffer)\n",
    "                buffer = []\n",
    "                current_key = \"\"\n",
    "\n",
    "            if lab == \"B-VERB\":\n",
    "                if current:\n",
    "                    slots.append(current)\n",
    "                current = {\"intent\": tok}\n",
    "            elif lab == \"B-OBJ\":\n",
    "                current_key = \"object\"\n",
    "                buffer = [tok]\n",
    "            elif lab == \"B-LOC\":\n",
    "                current_key = \"location\"\n",
    "                buffer = [tok]\n",
    "            elif lab == \"B-FILTER\":\n",
    "                current_key = \"filter\"\n",
    "                buffer = [tok]\n",
    "            elif lab == \"B-ATTR\":\n",
    "                current_key = \"attribute\"\n",
    "                buffer = [tok]\n",
    "            elif lab == \"B-PERSON\":\n",
    "                # Si ya hay intent, se asume persona es recipient/destinatario\n",
    "                if \"intent\" in current and current[\"intent\"] in [\"bring\", \"deliver\", \"give\"]:\n",
    "                    current[\"recipient\"] = tok\n",
    "                else:\n",
    "                    current_key = \"person\"\n",
    "                    buffer = [tok]\n",
    "            elif lab == \"B-CONTENT\":\n",
    "                current_key = \"content\"\n",
    "                buffer = [tok]\n",
    "\n",
    "        elif lab.startswith(\"I-\"):\n",
    "            # Seguir acumulando la frase compuesta\n",
    "            buffer.append(tok)\n",
    "\n",
    "    # Guardar último buffer si quedó algo\n",
    "    if current_key and buffer:\n",
    "        current[current_key] = \" \".join(buffer)\n",
    "\n",
    "    if current:\n",
    "        slots.append(current)\n",
    "\n",
    "    return slots\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_file = \"/content/drive/My Drive/LLM/dataset.jsonl\"  # tu archivo original\n",
    "    output_file = \"/content/drive/My Drive/LLM/dataset_with_slots.jsonl\"  # archivo nuevo\n",
    "\n",
    "    with open(input_file, \"r\") as f_in, open(output_file, \"w\") as f_out:\n",
    "        for line in f_in:\n",
    "            sample = json.loads(line)\n",
    "            tokens = sample[\"tokens\"]\n",
    "            labels = sample[\"labels\"]\n",
    "            slots = extract_slots(tokens, labels)\n",
    "            sample[\"slots\"] = slots\n",
    "            f_out.write(json.dumps(sample) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Guardado: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1751520359420,
     "user": {
      "displayName": "juan andres rivera gutierrez",
      "userId": "00082920026588442285"
     },
     "user_tz": 240
    },
    "id": "4bZTHn-ov3-4",
    "outputId": "f4c38601-9974-4654-a1ba-c28b85d03b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Intents encontrados y frecuencia:\n",
      "tell: 412\n",
      "follow: 111\n",
      "bring: 73\n",
      "look: 73\n",
      "meet: 72\n",
      "find: 71\n",
      "take: 68\n",
      "locate: 67\n",
      "give: 67\n",
      "get: 61\n",
      "deliver: 61\n",
      "answer: 60\n",
      "navigate: 50\n",
      "grasp: 49\n",
      "go: 45\n",
      "say: 40\n",
      "fetch: 39\n",
      "escort: 36\n",
      "put: 30\n",
      "lead: 26\n",
      "place: 25\n",
      "guide: 23\n",
      "introduce: 18\n",
      "greet: 12\n",
      "salute: 12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 📌 Ruta a tu archivo JSONL en tu Drive\n",
    "input_file = \"/content/drive/My Drive/LLM/dataset_with_slots.jsonl\"\n",
    "\n",
    "# Contador de intents\n",
    "intent_counter = Counter()\n",
    "\n",
    "with open(input_file) as fin:\n",
    "    for line in fin:\n",
    "        ex = json.loads(line)\n",
    "        slots = ex.get(\"slots\", [])\n",
    "        for s in slots:\n",
    "            intent = s.get(\"intent\", \"UNKNOWN\")\n",
    "            intent_counter[intent] += 1\n",
    "\n",
    "print(\"📌 Intents encontrados y frecuencia:\")\n",
    "for intent, count in intent_counter.most_common():\n",
    "    print(f\"{intent}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1751525765868,
     "user": {
      "displayName": "juan andres rivera gutierrez",
      "userId": "00082920026588442285"
     },
     "user_tz": 240
    },
    "id": "-RAuzwGaF65q",
    "outputId": "2d5407a4-83d1-4c0e-cc4d-8a1e345808d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset con planes robustos listo: /content/drive/My Drive/LLM/dataset_with_slots_and_plan.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "POSE_WORDS = [\n",
    "    \"lying\", \"sitting\", \"standing\", \"walking\", \"waiting\",\n",
    "    \"talking\", \"waving\", \"pointing\", \"raising\"\n",
    "]\n",
    "\n",
    "def fix_slots(slots):\n",
    "    \"\"\"Asegura que bring, deliver o give tengan grasp si falta\"\"\"\n",
    "    new_slots = []\n",
    "    for s in slots:\n",
    "        if s[\"intent\"] in [\"give\", \"deliver\", \"bring\"]:\n",
    "            if \"object\" in s and \"location\" in s:\n",
    "                if not any(sl[\"intent\"] == \"grasp\" for sl in slots):\n",
    "                    new_slots.append({\n",
    "                        \"intent\": \"grasp\",\n",
    "                        \"object\": s[\"object\"],\n",
    "                        \"location\": s[\"location\"]\n",
    "                    })\n",
    "        new_slots.append(s)\n",
    "    return new_slots\n",
    "\n",
    "def slots_to_plan(slots):\n",
    "    plan_steps = []\n",
    "    last_location = \"\"\n",
    "    last_object = \"\"\n",
    "    last_person = \"\"\n",
    "\n",
    "    for s in slots:\n",
    "        intent = s.get(\"intent\", \"\").lower()\n",
    "        obj = s.get(\"object\", \"\")\n",
    "        loc = s.get(\"location\", \"\")\n",
    "        content = s.get(\"content\", \"\")\n",
    "        person = s.get(\"person\", \"\")\n",
    "        filter_ = s.get(\"filter\", \"\")\n",
    "        attr = s.get(\"attribute\", \"\")\n",
    "        recipient = s.get(\"recipient\", \"\")\n",
    "\n",
    "        if loc: last_location = loc\n",
    "        if obj: last_object = obj\n",
    "        if person: last_person = person\n",
    "\n",
    "        # Si es ir o navegar\n",
    "        if intent in [\"navigate\", \"go\"]:\n",
    "            plan_steps.append(f\"navigate to {loc}\")\n",
    "\n",
    "        # Si hay location en otra acción => navegar primero\n",
    "        elif loc and intent not in [\"navigate\", \"go\"]:\n",
    "            plan_steps.append(f\"navigate to {loc}\")\n",
    "\n",
    "        # === Localizar ===\n",
    "        if intent in [\"locate\", \"find\", \"look\", \"fetch\"]:\n",
    "            if obj:\n",
    "                step = f\"detect_object {obj}\"\n",
    "                if loc:\n",
    "                    step += f\" at {loc}\"\n",
    "                plan_steps.append(step)\n",
    "            elif person or filter_:\n",
    "                step = \"detect_person\"\n",
    "                if filter_:\n",
    "                    if filter_ in POSE_WORDS:\n",
    "                        step += f\" with pose {filter_}\"\n",
    "                    else:\n",
    "                        step += f\" with {filter_}\"\n",
    "                if loc:\n",
    "                    step += f\" at {loc}\"\n",
    "                plan_steps.append(step)\n",
    "            elif attr:\n",
    "                step = f\"detect_object {attr}\"\n",
    "                if loc:\n",
    "                    step += f\" at {loc}\"\n",
    "                step += \" -> describe_property\"\n",
    "                plan_steps.append(step)\n",
    "\n",
    "        # === Agarrar ===\n",
    "        elif intent in [\"get\", \"grasp\", \"take\"]:\n",
    "            plan_steps.append(\"grasp\")\n",
    "\n",
    "        # === Traer o dar ===\n",
    "        elif intent in [\"bring\", \"deliver\", \"give\"]:\n",
    "            if not obj:\n",
    "                obj = last_object\n",
    "            step = \"deliver\"\n",
    "            if obj: step += f\" {obj}\"\n",
    "            # Por sentido: navega hasta receptor u operador si hay persona o recipient\n",
    "            target = person or recipient or \"operator\"\n",
    "            step += f\" to {target}\"\n",
    "            plan_steps.append(step)\n",
    "\n",
    "        # === Poner ===\n",
    "        elif intent in [\"put\", \"place\"]:\n",
    "            step = \"place\"\n",
    "            if obj: step += f\" {obj}\"\n",
    "            if loc: step += f\" at {loc}\"\n",
    "            plan_steps.append(step)\n",
    "\n",
    "        # === Responder ===\n",
    "        elif intent == \"answer\":\n",
    "            if person or filter_:\n",
    "                step = \"detect_person\"\n",
    "                if filter_ in POSE_WORDS:\n",
    "                    step += f\" with pose {filter_}\"\n",
    "                elif filter_:\n",
    "                    step += f\" with {filter_}\"\n",
    "                if loc:\n",
    "                    step += f\" at {loc}\"\n",
    "                plan_steps.append(step)\n",
    "            plan_steps.append(\"navigate to operator -> answer_question\")\n",
    "\n",
    "        # === Presentar ===\n",
    "        elif intent == \"introduce\":\n",
    "            plan_steps.append(\"introduce_person\")\n",
    "\n",
    "        # === Saludo o decir ===\n",
    "        elif intent in [\"greet\", \"salute\", \"say\"]:\n",
    "            step = \"greet_person\"\n",
    "            if person:\n",
    "                step += f\" {person}\"\n",
    "            if filter_:\n",
    "                step += f\" with {filter_}\"\n",
    "            if loc:\n",
    "                step += f\" at {loc}\"\n",
    "            plan_steps.append(step)\n",
    "\n",
    "        # === Guiar, acompañar ===\n",
    "        elif intent in [\"guide\", \"lead\", \"escort\", \"follow\"]:\n",
    "            step = \"guide_person\"\n",
    "            if person:\n",
    "                step += f\" {person}\"\n",
    "            if loc:\n",
    "                step += f\" to {loc}\"\n",
    "            plan_steps.append(step)\n",
    "\n",
    "        # === Encontrar ===\n",
    "        elif intent == \"meet\":\n",
    "            step = \"meet_person\"\n",
    "            if person:\n",
    "                step += f\" {person}\"\n",
    "            if loc:\n",
    "                step += f\" at {loc}\"\n",
    "            plan_steps.append(step)\n",
    "\n",
    "        # === Decir ===\n",
    "        elif intent == \"tell\":\n",
    "            if attr:\n",
    "                step = f\"detect_object {attr}\"\n",
    "                if obj:\n",
    "                    step += f\" {obj}\"\n",
    "                if loc:\n",
    "                    step += f\" at {loc}\"\n",
    "                step += \" -> describe_property\"\n",
    "                plan_steps.append(step)\n",
    "                plan_steps.append(\"navigate to operator -> tell\")\n",
    "            elif content:\n",
    "                plan_steps.append(f\"tell {content}\")\n",
    "            elif person or filter_:\n",
    "                step = \"detect_person\"\n",
    "                if filter_ in POSE_WORDS:\n",
    "                    step += f\" with pose {filter_}\"\n",
    "                elif filter_:\n",
    "                    step += f\" with {filter_}\"\n",
    "                if loc:\n",
    "                    step += f\" at {loc}\"\n",
    "                step += \" -> describe_property\"\n",
    "                plan_steps.append(step)\n",
    "                plan_steps.append(\"navigate to operator -> tell\")\n",
    "            elif obj:\n",
    "                step = f\"count {obj}\"\n",
    "                if loc:\n",
    "                    step += f\" at {loc}\"\n",
    "                plan_steps.append(step)\n",
    "                plan_steps.append(\"navigate to operator -> tell\")\n",
    "\n",
    "    return \" -> \".join(plan_steps)\n",
    "\n",
    "def main():\n",
    "    input_file = \"/content/drive/My Drive/LLM/dataset_with_slots.jsonl\"\n",
    "    output_file = \"/content/drive/My Drive/LLM/dataset_with_slots_and_plan.jsonl\"\n",
    "\n",
    "    with open(input_file) as fin, open(output_file, \"w\") as fout:\n",
    "        for line in fin:\n",
    "            ex = json.loads(line)\n",
    "            slots_fixed = fix_slots(ex[\"slots\"])\n",
    "            ex[\"plan\"] = slots_to_plan(slots_fixed)\n",
    "            fout.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Dataset con planes robustos listo: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1751525801350,
     "user": {
      "displayName": "juan andres rivera gutierrez",
      "userId": "00082920026588442285"
     },
     "user_tz": 240
    },
    "id": "uNAKHTzLR-h-",
    "outputId": "6dcae2f0-24b8-4d62-8fd2-ffa29a61db67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /content/drive/My Drive/LLM/dataset_for_t5.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "input_file = \"/content/drive/My Drive/LLM/dataset_with_slots_and_plan.jsonl\"\n",
    "output_file = \"/content/drive/My Drive/LLM/dataset_for_t5.csv\"\n",
    "\n",
    "with open(input_file, \"r\") as f_in, open(output_file, \"w\", newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow([\"input\", \"output\"])  # CSV header\n",
    "\n",
    "    for line in f_in:\n",
    "        sample = json.loads(line)\n",
    "        slots = sample[\"slots\"]\n",
    "        plan = sample[\"plan\"]\n",
    "\n",
    "        slot_str = \" \".join(\n",
    "            [f\"intent: {s.get('intent', '')}\"\n",
    "             + (f\" object: {s['object']}\" if 'object' in s else '')\n",
    "             + (f\" location: {s['location']}\" if 'location' in s else '')\n",
    "             + (f\" filter: {s['filter']}\" if 'filter' in s else '')\n",
    "             + (f\" recipient: {s['recipient']}\" if 'recipient' in s else '')\n",
    "             + (f\" attribute: {s['attribute']}\" if 'attribute' in s else '')\n",
    "             + (f\" content: {s['content']}\" if 'content' in s else '')\n",
    "             + (f\" person: {s['person']}\" if 'person' in s else '')\n",
    "             for s in slots]\n",
    "        )\n",
    "\n",
    "        writer.writerow([slot_str.strip(), plan])\n",
    "\n",
    "print(f\"✅ Saved: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871,
     "referenced_widgets": [
      "ad926bbd6e794f929f881500c1c72b7a",
      "df948e4ca717407baf38cba220f6e27d",
      "4b7e2914ab8f421796b3f119b2e0ea2a",
      "ca7f272f306142f3bdf3e3736cbb66bc",
      "1bc8572803b849ef98a8d2e040208a49",
      "1a89dd620d7c4358885de2e89582d978",
      "22e731843ca748ddbad01fca2102deb3",
      "3d8227ae122948708d1e38f0f2b29676",
      "1077630d5bfb4683907c52dff33410f5",
      "d32ba6cd2a9b4a149ef69374e901ec95",
      "c7e4f4430c3f4dccb2c3bda9365047f6",
      "b828ac2b2000441cbef63a7d335f2ec1",
      "029b5f3f78f448e28f3e1a48774024dd",
      "00398b7c75bd4502a4b3981c19574969",
      "215d2f78a34540078aa02b8795a1d744",
      "c66074253d6043d8abf45686347a70e4",
      "961e85b24d1c4533a34080896de372cb",
      "8416afbba7de4d3f923fe88087238f38",
      "0de72077e84344478c8c129167c4a95e",
      "384a17f37c5a45569b4e31c49c7544fa",
      "924f73003167435dbbc4005f59a2747f",
      "13784887976a4e3d9340eba262f50cdf"
     ]
    },
    "executionInfo": {
     "elapsed": 83158,
     "status": "ok",
     "timestamp": 1751525928589,
     "user": {
      "displayName": "juan andres rivera gutierrez",
      "userId": "00082920026588442285"
     },
     "user_tz": 240
    },
    "id": "KVoW9dfXSHfS",
    "outputId": "552e35e0-79bf-479b-b1c6-26ccc921cd68"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad926bbd6e794f929f881500c1c72b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b828ac2b2000441cbef63a7d335f2ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-50-2631434905.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 01:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.085200</td>\n",
       "      <td>0.756678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.410688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>0.221978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.145785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>0.080794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.045745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>0.033285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0.026813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.023045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.018881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.017439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.015637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.014167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.012746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.012206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.010863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.010706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.010739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.010573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/content/drive/My Drive/LLM//t5_plan_model/tokenizer_config.json',\n",
       " '/content/drive/My Drive/LLM//t5_plan_model/special_tokens_map.json',\n",
       " '/content/drive/My Drive/LLM//t5_plan_model/spiece.model',\n",
       " '/content/drive/My Drive/LLM//t5_plan_model/added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"/content/drive/My Drive/LLM/dataset_for_t5.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split train/valid\n",
    "ds = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"input\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(example[\"output\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "ds_enc = ds.map(preprocess, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./t5_plan_model\",\n",
    "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
    "    eval_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=200,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_enc[\"train\"],\n",
    "    eval_dataset=ds_enc[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"/content/drive/My Drive/LLM//t5_plan_model\")\n",
    "tokenizer.save_pretrained(\"/content/drive/My Drive/LLM//t5_plan_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1751525958922,
     "user": {
      "displayName": "juan andres rivera gutierrez",
      "userId": "00082920026588442285"
     },
     "user_tz": 240
    },
    "id": "oWb_wlch-VaD",
    "outputId": "7d2fe137-5974-4953-cd0f-41543b719f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INPUT SLOTS ===\n",
      "intent: locate object: cans location: bedroom intent: get intent: deliver recipient: person\n",
      "\n",
      "=== GENERATED PLAN ===\n",
      "navigate to bedroom -> detect_object cans at bedroom -> deliver cans to person\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 📂 Ruta de tu modelo entrenado:\n",
    "MODEL_DIR = \"/content/drive/My Drive/LLM/t5_plan_model\"\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR)\n",
    "\n",
    "def generate_plan(slot_input):\n",
    "    input_ids = tokenizer(slot_input, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=128)\n",
    "    plan = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return plan\n",
    "\n",
    "# 🧪 Ejemplo de prueba:\n",
    "example_slots = \"intent: locate object: cans location: bedroom intent: get intent: deliver recipient: person\"\n",
    "plan = generate_plan(example_slots)\n",
    "\n",
    "print(\"=== INPUT SLOTS ===\")\n",
    "print(example_slots)\n",
    "print(\"\\n=== GENERATED PLAN ===\")\n",
    "print(plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751523621057,
     "user": {
      "displayName": "juan andres rivera gutierrez",
      "userId": "00082920026588442285"
     },
     "user_tz": 240
    },
    "id": "a941tjTS-1T_",
    "outputId": "b1fc1733-ec90-449c-ab72-036676b880b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/usr/local/lib/python3.11/dist-packages/setuptools/_vendor', '/root/.ipython', '/tmp/tmpmocbt8md', '/content/drive/My Drive/LLM/CommandGenerator/src', '/content/drive/My Drive/LLM/CommandGenerator/src']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/drive/My Drive/LLM/CommandGenerator/src\")\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_VaWTve-2q9",
    "outputId": "9b269578-f363-4858-9e81-35c594057a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OPTIONS ===\n",
      "'1': Any GPSR command\n",
      "'2': GPSR no manipulation\n",
      "'3': GPSR manipulation\n",
      "'4': EGPSR multi-task (5 tasks)\n",
      "'q': Quit\n",
      "\n",
      "Select option: 1\n",
      "\n",
      "=== COMMAND ===\n",
      "go to the workshop then locate the waving person and answer a question\n",
      "\n",
      "\n",
      "=== BERT INFERENCE ===\n",
      "TOKEN\tLABEL\n",
      "[CLS]\tO\n",
      "go\tB-VERB\n",
      "to\tO\n",
      "the\tO\n",
      "workshop\tB-LOC\n",
      "then\tO\n",
      "locate\tB-VERB\n",
      "the\tO\n",
      "waving\tB-FILTER\n",
      "person\tB-PERSON\n",
      "and\tO\n",
      "answer\tB-VERB\n",
      "a\tO\n",
      "question\tB-CONTENT\n",
      "[SEP]\tI-FILTER\n",
      "----------------------\n",
      "\n",
      "=== SLOTS INPUT FOR T5 ===\n",
      "intent: go location: workshop | intent: locate filter: waving person: person | intent: answer content: question\n",
      "\n",
      "=== PLAN GENERATED ===\n",
      "navigate to workshop | intent -> detect_person with waving at workshop | intent -> answer_question\n",
      "------------------------------\n",
      "Select option: 1\n",
      "\n",
      "=== COMMAND ===\n",
      "tell me what is the lightest object on the counter\n",
      "\n",
      "\n",
      "=== BERT INFERENCE ===\n",
      "TOKEN\tLABEL\n",
      "[CLS]\tO\n",
      "tell\tB-VERB\n",
      "me\tI-VERB\n",
      "what\tI-VERB\n",
      "is\tO\n",
      "the\tO\n",
      "light\tB-ATTR\n",
      "##est\tB-ATTR\n",
      "object\tO\n",
      "on\tB-LOC\n",
      "the\tI-LOC\n",
      "counter\tI-LOC\n",
      "[SEP]\tI-FILTER\n",
      "----------------------\n",
      "\n",
      "=== SLOTS INPUT FOR T5 ===\n",
      "intent: tell attribute: lightest location: on the counter\n",
      "\n",
      "=== PLAN GENERATED ===\n",
      "navigate to on the counter -> detect_object lightest at on the counter -> describe_property -> navigate to operator -> tell\n",
      "------------------------------\n",
      "Select option: 1\n",
      "\n",
      "=== COMMAND ===\n",
      "tell me how many cup there are on the desk\n",
      "\n",
      "\n",
      "=== BERT INFERENCE ===\n",
      "TOKEN\tLABEL\n",
      "[CLS]\tO\n",
      "tell\tB-VERB\n",
      "me\tI-VERB\n",
      "how\tI-VERB\n",
      "many\tI-VERB\n",
      "cup\tB-OBJ\n",
      "there\tO\n",
      "are\tO\n",
      "on\tB-LOC\n",
      "the\tI-LOC\n",
      "desk\tI-LOC\n",
      "[SEP]\tI-FILTER\n",
      "----------------------\n",
      "\n",
      "=== SLOTS INPUT FOR T5 ===\n",
      "intent: tell object: cup location: on the desk\n",
      "\n",
      "=== PLAN GENERATED ===\n",
      "navigate to on the desk -> count cup at on the desk -> navigate to operator -> tell\n",
      "------------------------------\n",
      "Select option: 1\n",
      "\n",
      "=== COMMAND ===\n",
      "grasp a snack from the garden table and give it to me\n",
      "\n",
      "\n",
      "=== BERT INFERENCE ===\n",
      "TOKEN\tLABEL\n",
      "[CLS]\tO\n",
      "grasp\tB-VERB\n",
      "a\tO\n",
      "snack\tB-OBJ\n",
      "from\tO\n",
      "the\tO\n",
      "garden\tB-LOC\n",
      "table\tI-LOC\n",
      "and\tO\n",
      "give\tB-VERB\n",
      "it\tO\n",
      "to\tO\n",
      "me\tB-PERSON\n",
      "[SEP]\tI-FILTER\n",
      "----------------------\n",
      "\n",
      "=== SLOTS INPUT FOR T5 ===\n",
      "intent: grasp object: snack location: garden table | intent: give person: me\n",
      "\n",
      "=== PLAN GENERATED ===\n",
      "navigate to garden table -> grasp -> navigate to garden table -> deliver snack to me\n",
      "------------------------------\n",
      "Select option: 2\n",
      "\n",
      "=== COMMAND ===\n",
      "say hello to the person wearing a blue blouse in the study  and guide them to the workshop\n",
      "\n",
      "\n",
      "=== BERT INFERENCE ===\n",
      "TOKEN\tLABEL\n",
      "[CLS]\tO\n",
      "say\tB-VERB\n",
      "hello\tI-VERB\n",
      "to\tI-VERB\n",
      "the\tO\n",
      "person\tB-PERSON\n",
      "wearing\tB-FILTER\n",
      "a\tO\n",
      "blue\tB-PERSON\n",
      "blouse\tI-PERSON\n",
      "in\tO\n",
      "the\tO\n",
      "study\tB-LOC\n",
      "and\tO\n",
      "guide\tB-VERB\n",
      "them\tO\n",
      "to\tO\n",
      "the\tO\n",
      "workshop\tB-LOC\n",
      "[SEP]\tI-FILTER\n",
      "----------------------\n",
      "\n",
      "=== SLOTS INPUT FOR T5 ===\n",
      "intent: say person: blue filter: wearing location: study | intent: guide location: workshop\n",
      "\n",
      "=== PLAN GENERATED ===\n",
      "navigate to study | navigate to workshop -> guide_person to workshop\n",
      "------------------------------\n",
      "Select option: 3\n",
      "\n",
      "=== COMMAND ===\n",
      "look for a bottle in the terrace  then take it and bring it to me\n",
      "\n",
      "\n",
      "=== BERT INFERENCE ===\n",
      "TOKEN\tLABEL\n",
      "[CLS]\tO\n",
      "look\tB-VERB\n",
      "for\tI-VERB\n",
      "a\tO\n",
      "bottle\tB-OBJ\n",
      "in\tO\n",
      "the\tO\n",
      "terrace\tB-LOC\n",
      "then\tO\n",
      "take\tB-VERB\n",
      "it\tO\n",
      "and\tO\n",
      "bring\tB-VERB\n",
      "it\tO\n",
      "to\tO\n",
      "me\tB-PERSON\n",
      "[SEP]\tI-FILTER\n",
      "----------------------\n",
      "\n",
      "=== SLOTS INPUT FOR T5 ===\n",
      "intent: look object: bottle location: terrace | intent: take | intent: bring person: me\n",
      "\n",
      "=== PLAN GENERATED ===\n",
      "navigate to terrace -> grasp -> navigate to terrace -> bring bottle to me\n",
      "------------------------------\n",
      "Select option: 4\n",
      "\n",
      "=== COMMAND ===\n",
      "1) There is a person at the office, their request is:\n",
      "\t tell me how many standing persons are in the bedroom \n",
      "2) There is a person at the living room, their request is:\n",
      "\t look for a book in the bedroom  then fetch it and give it to the person raising their right arm in the bedroom \n",
      "3) Put an object on the floor in the entrance \n",
      "4) The snack is at the side table\n",
      "5) The bowl is at the shelf\n",
      "\n",
      "\n",
      "=== BERT INFERENCE ===\n",
      "TOKEN\tLABEL\n",
      "[CLS]\tO\n",
      "1\tB-PERSON\n",
      ")\tO\n",
      "there\tO\n",
      "is\tO\n",
      "a\tO\n",
      "person\tB-PERSON\n",
      "at\tO\n",
      "the\tO\n",
      "office\tB-LOC\n",
      ",\tO\n",
      "their\tO\n",
      "request\tB-CONTENT\n",
      "is\tO\n",
      ":\tO\n",
      "tell\tB-VERB\n",
      "me\tI-VERB\n",
      "how\tI-VERB\n",
      "many\tI-VERB\n",
      "standing\tB-FILTER\n",
      "persons\tO\n",
      "are\tO\n",
      "in\tO\n",
      "the\tO\n",
      "bedroom\tB-LOC\n",
      "2\tB-CONTENT\n",
      ")\tO\n",
      "there\tO\n",
      "is\tO\n",
      "a\tO\n",
      "person\tB-PERSON\n",
      "at\tO\n",
      "the\tO\n",
      "living\tB-LOC\n",
      "room\tI-LOC\n",
      ",\tO\n",
      "their\tO\n",
      "request\tB-CONTENT\n",
      "is\tO\n",
      ":\tO\n",
      "look\tB-VERB\n",
      "for\tI-VERB\n",
      "a\tO\n",
      "book\tB-OBJ\n",
      "in\tO\n",
      "the\tO\n",
      "bedroom\tB-LOC\n",
      "then\tO\n",
      "fetch\tB-VERB\n",
      "it\tO\n",
      "and\tO\n",
      "give\tB-VERB\n",
      "it\tO\n",
      "to\tO\n",
      "the\tO\n",
      "person\tB-PERSON\n",
      "raising\tB-FILTER\n",
      "their\tO\n",
      "right\tB-FILTER\n",
      "arm\tI-FILTER\n",
      "in\tO\n",
      "the\tO\n",
      "bedroom\tB-LOC\n",
      "3\tO\n",
      ")\tO\n",
      "put\tB-VERB\n",
      "an\tO\n",
      "object\tO\n",
      "on\tB-LOC\n",
      "the\tI-LOC\n",
      "floor\tI-LOC\n",
      "in\tO\n",
      "the\tO\n",
      "entrance\tB-LOC\n",
      "4\tB-OBJ\n",
      ")\tO\n",
      "the\tO\n",
      "snack\tB-OBJ\n",
      "is\tO\n",
      "at\tO\n",
      "the\tO\n",
      "side\tB-LOC\n",
      "table\tI-LOC\n",
      "5\tB-OBJ\n",
      ")\tO\n",
      "the\tO\n",
      "bowl\tB-OBJ\n",
      "is\tO\n",
      "at\tO\n",
      "the\tO\n",
      "shelf\tB-LOC\n",
      "[SEP]\tI-FILTER\n",
      "----------------------\n",
      "\n",
      "=== SLOTS INPUT FOR T5 ===\n",
      "person: person location: office content: request | intent: tell filter: standing location: living room content: request person: person | intent: look object: book location: bedroom | intent: fetch | intent: give person: person filter: right location: bedroom | intent: put location: shelf object: bowl\n",
      "\n",
      "=== PLAN GENERATED ===\n",
      "navigate to living room -> detect_person with pose at living room -> navigate to bedroom -> grasp -> navigate to shelf -> deliver book to bowl\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from robocupathome_generator.gpsr_commands import CommandGenerator\n",
    "from robocupathome_generator.egpsr_commands import EgpsrCommandGenerator\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# === LOAD MODELS ===\n",
    "BERT_PATH = \"/content/drive/My Drive/LLM/bert_token_classification_model/checkpoint-339\"\n",
    "T5_PATH = \"/content/drive/My Drive/LLM/t5_plan_model\"\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(BERT_PATH)\n",
    "bert_model = BertForTokenClassification.from_pretrained(BERT_PATH)\n",
    "bert_model.eval()\n",
    "id2label = bert_model.config.id2label\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_PATH)\n",
    "\n",
    "# === BERT inference ===\n",
    "def run_inference(command):\n",
    "    words = command.strip().split()\n",
    "    if not words:\n",
    "        print(\"\\n[⚠️] Empty command.\")\n",
    "        return [], []\n",
    "\n",
    "    inputs = bert_tokenizer(words, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    preds = outputs.logits.argmax(dim=-1).squeeze().tolist()\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "\n",
    "    print(\"\\n=== BERT INFERENCE ===\")\n",
    "    print(\"TOKEN\\tLABEL\")\n",
    "    for t, p in zip(tokens, preds):\n",
    "        print(f\"{t}\\t{id2label.get(p, 'O')}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "    labels = [id2label.get(p, \"O\") for p in preds]\n",
    "    return tokens, labels\n",
    "\n",
    "# === Combine subwords ===\n",
    "def combine_subwords(tokens, labels):\n",
    "    new_tokens = []\n",
    "    new_labels = []\n",
    "    skip = False\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "\n",
    "        token = tokens[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        if token.startswith(\"##\") and new_tokens:\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        elif i + 1 < len(tokens) and tokens[i + 1].startswith(\"##\"):\n",
    "            new_tokens.append(token + tokens[i + 1][2:])\n",
    "            new_labels.append(label)\n",
    "            skip = True\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_tokens, new_labels\n",
    "\n",
    "# === Slots extractor ===\n",
    "def extract_slots(tokens, labels):\n",
    "    slots = []\n",
    "    current = {}\n",
    "\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab.startswith(\"B-VERB\"):\n",
    "            if current:\n",
    "                slots.append(current)\n",
    "            current = {\"intent\": tok}\n",
    "        elif lab.startswith(\"B-OBJ\"):\n",
    "            current[\"object\"] = tok\n",
    "        elif lab.startswith(\"B-LOC\"):\n",
    "            current[\"location\"] = tok\n",
    "        elif lab.startswith(\"I-LOC\") and \"location\" in current:\n",
    "            current[\"location\"] += \" \" + tok\n",
    "        elif lab.startswith(\"B-ATTR\"):\n",
    "            current[\"attribute\"] = tok\n",
    "        elif lab.startswith(\"B-FILTER\"):\n",
    "            current[\"filter\"] = tok\n",
    "        elif lab.startswith(\"B-PERSON\"):\n",
    "            current[\"person\"] = tok\n",
    "        elif lab.startswith(\"B-CONTENT\"):\n",
    "            current[\"content\"] = tok\n",
    "\n",
    "    if current:\n",
    "        slots.append(current)\n",
    "    return slots\n",
    "\n",
    "# === Clean slots ===\n",
    "def clean_slots(slots):\n",
    "    for s in slots:\n",
    "        attr = s.get(\"attribute\", \"\")\n",
    "        if attr.startswith(\"##\"):\n",
    "            s[\"attribute\"] = attr.replace(\"##\", \"\")\n",
    "        if \"filter\" in s and \"intent\" not in s:\n",
    "            s[\"intent\"] = \"locate\"\n",
    "    return slots\n",
    "\n",
    "def slots_to_string(slots):\n",
    "    slots = clean_slots(slots)\n",
    "    return \" | \".join(\" \".join(f\"{k}: {v}\" for k, v in s.items()) for s in slots)\n",
    "\n",
    "# === T5 plan ===\n",
    "def t5_generate_plan(slots_input):\n",
    "    enc = t5_tokenizer(slots_input, return_tensors=\"pt\")\n",
    "    out = t5_model.generate(**enc, max_length=50)\n",
    "    plan = t5_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return plan\n",
    "\n",
    "# === Data utils ===\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        return f.read()\n",
    "\n",
    "def parse_names(data):\n",
    "    return re.findall(r\"\\|\\s*([A-Za-z]+)\\s*\\|\", data, re.DOTALL)[1:]\n",
    "\n",
    "def parse_locations(data):\n",
    "    parsed = re.findall(r\"\\|\\s*([0-9]+)\\s*\\|\\s*([A-Za-z,\\s,\\(\\)]+)\\|\", data, re.DOTALL)\n",
    "    raw = [b.strip() for (_, b) in parsed]\n",
    "    plocs = [b.replace(\"(p)\", \"\").strip() for b in raw if \"(p)\" in b]\n",
    "    locs = [b.replace(\"(p)\", \"\").strip() for b in raw]\n",
    "    return locs, plocs\n",
    "\n",
    "def parse_rooms(data):\n",
    "    return re.findall(r\"\\|\\s*(\\w+ \\w*)\\s*\\|\", data, re.DOTALL)[1:]\n",
    "\n",
    "def parse_objects(data):\n",
    "    parsed = re.findall(r\"\\|\\s*(\\w+)\\s*\\|\", data, re.DOTALL)\n",
    "    objects = [o.replace(\"_\", \" \") for o in parsed if o != \"Objectname\"]\n",
    "    cats = re.findall(r\"# Class \\s*([\\w,\\s,\\(\\)]+)\\s*\", data, re.DOTALL)\n",
    "    cats = [c.replace(\"(\", \"\").replace(\")\", \"\").split() for c in cats]\n",
    "    plurals = [c[0].replace(\"_\", \" \") for c in cats]\n",
    "    singulars = [c[1].replace(\"_\", \" \") for c in cats]\n",
    "    return objects, plurals, singulars\n",
    "\n",
    "# === Mount your paths ===\n",
    "data_dir = \"/content/drive/My Drive/LLM/CommandGenerator\"\n",
    "names = parse_names(read_data(f\"{data_dir}/names/names.md\"))\n",
    "locs, plocs = parse_locations(read_data(f\"{data_dir}/maps/location_names.md\"))\n",
    "rooms = parse_rooms(read_data(f\"{data_dir}/maps/room_names.md\"))\n",
    "objs, cats_plural, cats_singular = parse_objects(read_data(f\"{data_dir}/objects/objects.md\"))\n",
    "\n",
    "gpsr_gen = CommandGenerator(names, locs, plocs, rooms, objs, cats_plural, cats_singular)\n",
    "egpsr_gen = EgpsrCommandGenerator(gpsr_gen)\n",
    "\n",
    "# === MAIN ===\n",
    "print(\"=== OPTIONS ===\")\n",
    "print(\"'1': Any GPSR command\")\n",
    "print(\"'2': GPSR no manipulation\")\n",
    "print(\"'3': GPSR manipulation\")\n",
    "print(\"'4': EGPSR multi-task (5 tasks)\")\n",
    "print(\"'q': Quit\\n\")\n",
    "\n",
    "while True:\n",
    "    option = input(\"Select option: \").strip()\n",
    "    if option == \"q\":\n",
    "        break\n",
    "\n",
    "    if option == \"1\":\n",
    "        command = gpsr_gen.generate_command_start(\"\")\n",
    "    elif option == \"2\":\n",
    "        command = gpsr_gen.generate_command_start(\"people\")\n",
    "    elif option == \"3\":\n",
    "        command = gpsr_gen.generate_command_start(\"objects\")\n",
    "    elif option == \"4\":\n",
    "        setups = egpsr_gen.generate_setup(5)\n",
    "        command = \"\\n\".join([f\"{i+1}) {t.task}\" for i, t in enumerate(setups)])\n",
    "    else:\n",
    "        print(\"[⚠️] Invalid option\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== COMMAND ===\\n{command}\\n\")\n",
    "\n",
    "    tokens, labels = run_inference(command)\n",
    "    tokens, labels = combine_subwords(tokens, labels)\n",
    "    slots = extract_slots(tokens, labels)\n",
    "    slots_str = slots_to_string(slots)\n",
    "    print(\"\\n=== SLOTS INPUT FOR T5 ===\")\n",
    "    print(slots_str)\n",
    "\n",
    "    plan = t5_generate_plan(slots_str)\n",
    "    print(\"\\n=== PLAN GENERATED ===\")\n",
    "    print(plan)\n",
    "    print(\"------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNJTIVg+iNBtAgfJIyRdzuW",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
