# ğŸ¤– Proyecto NLP RoboCup â€” TokenizaciÃ³n, ClasificaciÃ³n y PlanificaciÃ³n (BERT + T5)

Este repositorio contiene el desarrollo de un sistema de comprensiÃ³n y planificaciÃ³n de lenguaje natural aplicado al dominio **RoboCup@Home**, centrado en la generaciÃ³n de comandos estructurados para robots de servicio (por ejemplo, Pepper o Darwin-OP).  

El proyecto combina **clasificaciÃ³n de tokens (NER)** con **modelos de planificaciÃ³n secuencial (T5)**, permitiendo convertir instrucciones humanas en estructuras comprensibles por el robot.

---

## ğŸ“ Estructura del repositorio

```
.
â”œâ”€â”€ bert_token_classification_model/
â”‚   â””â”€â”€ checkpoint-339/              â†’ Pesos del modelo BERT fine-tuneado para clasificaciÃ³n de tokens
â”œâ”€â”€ t5_plan_model/
â”‚   â””â”€â”€ ...                          â†’ Pesos del modelo T5 para planificaciÃ³n
â”œâ”€â”€ Ruta.ipynb                       â†’ Notebook principal: flujo de entrenamiento y evaluaciÃ³n de modelos
â”œâ”€â”€ Tokenizar.ipynb                  â†’ Notebook de tokenizaciÃ³n y preprocesamiento de dataset
â”œâ”€â”€ dataset.jsonl                    â†’ Dataset base (formato JSON Lines)
â”œâ”€â”€ dataset_for_t5.csv               â†’ Dataset adaptado para entrenamiento del T5
â”œâ”€â”€ dataset_with_slots.jsonl         â†’ Dataset enriquecido con â€œslotsâ€ semÃ¡nticos
â”œâ”€â”€ dataset_with_slots_and_plan.jsonlâ†’ Dataset con slots + plan completo
â”œâ”€â”€ .gitattributes                   â†’ ConfiguraciÃ³n de Git LFS (para subir modelos grandes)
â”œâ”€â”€ requirements.txt                 â†’ Dependencias principales del proyecto
â””â”€â”€ README.md                        â†’ Este documento
```

---

## âš™ï¸ Requisitos e instalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/JuanRivera182003/tu-repo.git
cd tu-repo
```

### 2. Instalar dependencias

Crea el entorno virtual y luego instala las librerÃ­as necesarias:

```bash
python -m venv venv
source venv/bin/activate   # En Linux / macOS
venv\Scripts\activate      # En Windows

pip install -r requirements.txt
```

Si aÃºn no tienes el archivo `requirements.txt`, crea uno con este contenido:

```text
torch>=2.0.0
transformers>=4.38.0
datasets>=2.18.0
pandas
numpy
scikit-learn
tqdm
```

*(opcional)* si planeas visualizar o ejecutar notebooks:
```text
jupyter
notebook
```

---

## ğŸ“˜ DescripciÃ³n tÃ©cnica

### 1. TokenizaciÃ³n y ClasificaciÃ³n (BERT)
- Modelo base: `bert-base-cased`
- Tarea: **Token Classification / NER**
- Entrenado en los datasets JSONL con etiquetas de comandos RoboCup.
- Objetivo: identificar entidades como **acciÃ³n**, **objeto**, **ubicaciÃ³n**, **agente**, etc.

### 2. PlanificaciÃ³n Secuencial (T5)
- Modelo base: `t5-small`
- Tarea: **Conditional Text Generation**
- Entrada: texto anotado con los â€œslotsâ€ generados por el modelo BERT.
- Salida: **plan estructurado** para el robot (ej. `go(kitchen); grasp(cup); return(table)`).

---

## ğŸ§© Flujo de trabajo

1. **Tokenizar y etiquetar texto** con `Tokenizar.ipynb`
   - Limpieza y divisiÃ³n del dataset.
   - Entrenamiento o carga del modelo BERT.
   - PredicciÃ³n de etiquetas por token.

2. **Generar dataset estructurado** (`dataset_with_slots.jsonl`)
   - A partir de las etiquetas anteriores, construir pares entradaâ€“salida para el modelo T5.

3. **Entrenar planificador (T5)** con `Ruta.ipynb`
   - Entrenamiento del modelo T5.
   - EvaluaciÃ³n y pruebas de generaciÃ³n de secuencias.

4. **Ejecutar inferencia final**
   ```python
   from transformers import AutoTokenizer, AutoModelForTokenClassification, T5Tokenizer, T5ForConditionalGeneration

   # === TokenizaciÃ³n ===
   tok = AutoTokenizer.from_pretrained("bert_token_classification_model/checkpoint-339")
   model = AutoModelForTokenClassification.from_pretrained("bert_token_classification_model/checkpoint-339")

   inputs = tok("Bring me the red cup from the kitchen", return_tensors="pt")
   outputs = model(**inputs)
   # Procesar logits para obtener etiquetas

   # === PlanificaciÃ³n ===
   t5_tok = T5Tokenizer.from_pretrained("t5_plan_model")
   t5_model = T5ForConditionalGeneration.from_pretrained("t5_plan_model")

   plan_in = "action: bring; object: cup; color: red; location: kitchen"
   plan_ids = t5_model.generate(t5_tok(plan_in, return_tensors="pt").input_ids)
   print(t5_tok.decode(plan_ids[0], skip_special_tokens=True))
   ```

---

## ğŸ¤– Referencia RoboCup â€” Generador de Comandos

Este trabajo se basa en el **Generador de Comandos NLP** del repositorio oficial de RoboCup@Home, disponible en:

ğŸ”— [https://github.com/RoboCupAtHome/command-generator](https://github.com/RoboCupAtHome/command-generator)

AllÃ­ se definiÃ³ la estructura base de **slots** y **plantillas de comandos** que fueron adaptadas y enriquecidas en este proyecto.

---

## ğŸ“Š Resultados esperados

- **Modelo BERT**: precisiÃ³n de etiquetado > 90% en entidades principales.  
- **Modelo T5**: generaciÃ³n coherente de planes en formato lÃ³gico ejecutable.  
- **IntegraciÃ³n**: permite pasar directamente de texto natural â†’ estructura semÃ¡ntica â†’ plan ejecutable por el robot.

---

## ğŸ§¾ CrÃ©ditos y licencia

**Autor principal:** Juan AndrÃ©s Rivera GutiÃ©rrez  
**InstituciÃ³n:** Universidad Santo TomÃ¡s â€“ USTA ğŸ‡¨ğŸ‡´ / Universidad AndrÃ©s Bello â€“ UNAB ğŸ‡¨ğŸ‡±  
**AÃ±o:** 2025  
**Licencia:** MIT License (ver archivo `LICENSE` si se incluye)

Basado en modelos y librerÃ­as de:
- [ğŸ¤— Hugging Face Transformers](https://huggingface.co/transformers/)
- [RoboCup@Home Command Generator](https://github.com/RoboCupAtHome/command-generator)

---

## ğŸ’¡ Notas adicionales

- Los notebooks fueron limpiados de metadatos para visualizar correctamente en GitHub.  
- Los modelos grandes usan [Git LFS](https://git-lfs.github.com/).  
- Puedes agregar una carpeta `scripts/` si deseas incluir pipelines de entrenamiento automatizados.  
- Se recomienda incluir:
  - `.gitignore`
  - `requirements.txt`
  - `LICENSE`
  - y opcionalmente `config.json` con parÃ¡metros de los modelos.

---

## âœ… PrÃ³ximos pasos

1. Subir el archivo `requirements.txt` al repo.  
2. (Opcional) Crear un `LICENSE` con tipo MIT.  
3. Agregar badges de estado (por ejemplo: versiÃ³n de Python, modelo usado, licencia).  
4. Si deseas despliegue automÃ¡tico en **Hugging Face Spaces** o **Colab**, puedo dejarte el `app.py` y `Space README.md` listos.
